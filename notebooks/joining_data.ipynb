{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join the data from Part 1 with the data from Part 2 to create a new dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "citybikes_berlin_df = pd.read_csv('berlin_nextbike_final.csv') # citybikes data\n",
    "fsq_berlin_df = pd.read_csv('fsq_berlin_df.csv', index_col=0) # foursquare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def latlon_bucket(lat, lon, precision=2):\n",
    "    return f\"{round(lat, precision)}_{round(lon, precision)}\" # function to round lat and lon to 2 decimal places and concatenate lat/lon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "citybikes_berlin_df['latlon_bucket'] = citybikes_berlin_df.apply(lambda row: latlon_bucket(row['latitude'], row['longitude']), axis=1) # apply function\n",
    "fsq_berlin_df['latlon_bucket'] = fsq_berlin_df.apply(lambda row: latlon_bucket(row['latitude'], row['longitude']), axis=1) # apply function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "berlin_merged_df = pd.merge(fsq_berlin_df, citybikes_berlin_df, how='left', on='latlon_bucket')\n",
    "berlin_merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for missing values\n",
    "\n",
    "total = berlin_merged_df.isnull().sum().sort_values(ascending=False)\n",
    "percent = (berlin_merged_df.isnull().sum()/berlin_merged_df.isnull().count()).sort_values(ascending=False)\n",
    "missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "missing_data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows where venue details weren't available\n",
    "\n",
    "berlin_merged_df = berlin_merged_df[berlin_merged_df['postcode'].notnull()]\n",
    "berlin_merged_df = berlin_merged_df[berlin_merged_df['address'].notnull()]\n",
    "berlin_merged_df = berlin_merged_df[berlin_merged_df['name_y'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final check for missing values \n",
    "\n",
    "total = berlin_merged_df.isnull().sum().sort_values(ascending=False)\n",
    "percent = (berlin_merged_df.isnull().sum()/berlin_merged_df.isnull().count()).sort_values(ascending=False)\n",
    "missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "missing_data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "berlin_merged_df = berlin_merged_df.drop(columns=['fsq_place_id','id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns for ease of reading\n",
    "\n",
    "berlin_merged_df.columns = [ 'venue_name','venue_category_name','venue_address','venue_postcode','venue_latitude','venue_longitude','distance_to_bike','latlon_bucket','bike_site_name','bike_site_latitude','bike_site_longitude','free_bikes','empty_slots','total_bikes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "berlin_merged_df.to_csv('berlin_merged_final.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide a visualization that you used as part of your EDA process. Explain the initial pattern or relationship you discoved through this visualization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate no distances are greater than 1000m or less than 0\n",
    "\n",
    "y = berlin_merged_df['distance_to_bike']\n",
    "plt.hist(y, bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate outliers in lat/lon which would imply data includes values outside of Berlin\n",
    "\n",
    "lat_boxplot = berlin_merged_df.boxplot(column=['venue_latitude','bike_site_latitude'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lon_boxplot = berlin_merged_df.boxplot(column=['venue_longitude','bike_site_longitude'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put all your results in an SQLite3 database (remember, SQLite stores its databases as files in your local machine - make sure to create your database in your project's data/ directory!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "from sqlite3 import Error\n",
    "\n",
    "def create_connection(path):\n",
    "    connection = None\n",
    "    try:\n",
    "        connection = sqlite3.connect(path)\n",
    "        print(\"Connection to SQLite DB successful\")\n",
    "    except Error as e:\n",
    "        print(f\"The error '{e}' occurred\")\n",
    "\n",
    "    return connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = create_connection(\"C:/Users/vlouc/OneDrive/Documents/GitHub/LHL-Stats-Project/data/sm_app.sqlite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_query(connection, query):\n",
    "    cursor = connection.cursor()\n",
    "    try:\n",
    "        cursor.execute(query)\n",
    "        connection.commit()\n",
    "        print(\"Query executed successfully\")\n",
    "    except Error as e:\n",
    "        print(f\"The error '{e}' occurred\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_fsq_table = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS berlin_fsq_data (\n",
    "  fsq_place_id TEXT,\n",
    "  name TEXT,\n",
    "  category_name TEXT,\n",
    "  address TEXT,\n",
    "  postcode TEXT,\n",
    "  latitude INTEGER,\n",
    "  longitude INTEGER,\n",
    "  distance INTEGER,\n",
    "  FOREIGN KEY (latlon_bucket) REFERENCES berlin_citybikes_data (latlon_bucket)\n",
    "\n",
    ");\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_citybikes_table = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS berlin_citybikes_data (\n",
    "id TEXT,\n",
    "name TEXT,\n",
    "latitude INTEGER,\n",
    "longitude INTEGER,\n",
    "free_bikes INTEGER,\n",
    "empty_slots INTEGER,\n",
    "total_bikes INTEGER,\n",
    "latlon_bucket INTEGER PRIMARY KEY\n",
    ");\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execute_query(connection, create_fsq_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execute_query(connection, create_citybikes_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fsq_berlin_df.to_sql('berlin_fsq_data', connection, if_exists='replace', index=False) # load fsq df to sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "citybikes_berlin_df.to_sql('berlin_citybikes_data', connection, if_exists='replace', index=False) # load citybikes df to sql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the data before and after the join to validate your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_all_fsq = \"SELECT * from berlin_fsq_data\"\n",
    "sqlite_test_fsq = pd.read_sql(select_all_fsq, connection)\n",
    "sqlite_test_fsq.shape == fsq_berlin_df.shape #confirm shape of sql extract matches original df shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_all_citybikes = \"SELECT * from berlin_citybikes_data\"\n",
    "sqlite_test_citybikes = pd.read_sql(select_all_citybikes, connection)\n",
    "sqlite_test_citybikes.shape == citybikes_berlin_df.shape #confirm shape of sql extract matches original df shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_merged_table = \"SELECT * from berlin_fsq_data fsq JOIN berlin_citybikes_data cb WHERE fsq.latlon_bucket = cb.latlon_bucket\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_sql(create_merged_table, connection).shape # matches shape of original berlin_merged_df +1 column because of duplicated latlon_bucket field"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LHL-Env",
   "language": "python",
   "name": "lhl-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
